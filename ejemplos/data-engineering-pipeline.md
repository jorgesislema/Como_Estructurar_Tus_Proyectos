
# Ejemplo: Pipeline de IngenierÃ­a de Datos Moderna (Dagster, dbt, Python)

Â¡Hola, ingenieros/as de datos! Esta plantilla de ejemplo estÃ¡ diseÃ±ada para estructurar proyectos de ingenierÃ­a de datos modernos, enfocados en la creaciÃ³n de pipelines **confiables, observables y testeables** para procesos ETL (Extract, Transform, Load) y ELT (Extract, Load, Transform).

Utilizamos **Dagster** como orquestador principal. Dagster es un framework Python moderno que te permite definir tus pipelines como cÃ³digo, con un fuerte Ã©nfasis en los **activos de datos (Software-Defined Assets)**, el testing y una interfaz de usuario (Dagit) excelente para desarrollo y monitorizaciÃ³n. Opcionalmente, integramos **dbt (data build tool)** para manejar las transformaciones basadas en SQL, un patrÃ³n muy comÃºn en arquitecturas ELT.

## Objetivo ğŸ¯

* Proporcionar una **estructura organizada y escalable** para proyectos de ingenierÃ­a de datos.
* Demostrar el uso de **Dagster** para orquestaciÃ³n, definiciÃ³n de activos y observabilidad.
* Mostrar cÃ³mo integrar **dbt** (opcionalmente) para transformaciones SQL dentro de un pipeline Dagster.
* Fomentar **buenas prÃ¡cticas** como la definiciÃ³n de assets, testing, y configuraciÃ³n basada en entorno.
* Servir como un **punto de partida prÃ¡ctico** y adaptable para tus pipelines.

## TecnologÃ­as Clave ğŸ› ï¸

* **Orquestador / DefiniciÃ³n de Activos:** [Dagster](https://dagster.io/)
* **TransformaciÃ³n SQL (Opcional):** [dbt (data build tool)](https://www.getdbt.com/) (ej: `dbt-core`, `dbt-postgres`)
* **TransformaciÃ³n Python:** [Pandas](https://pandas.pydata.org/) o [Polars](https://pola.rs/)
* **InteracciÃ³n con BD:** [SQLAlchemy](https://www.sqlalchemy.org/) (usado por Dagster/dbt para conectar)
* **GestiÃ³n de Dependencias:** [Poetry](https://python-poetry.org/) o [Hatch](https://hatch.pypa.io/) (vÃ­a `pyproject.toml`)
* **Testing:** [Pytest](https://pytest.org/)
* **(Opcional) Contenedores:** [Docker](https://www.docker.com/) & [Docker Compose](https://docs.docker.com/compose/)

## Estructura del Directorio ğŸ—ºï¸

La estructura estÃ¡ centrada alrededor del proyecto Dagster y opcionalmente un proyecto dbt:

* **`dagster_project/`**: El corazÃ³n de tu cÃ³digo Dagster. Contiene las definiciones de tus pipelines y activos.
    * **`repository.py`**: Punto de entrada clave. Define un `@repository` que agrupa tus activos, jobs, schedules, sensors, y recursos para que Dagster los descubra.
    * **`assets/`**: **Enfoque preferido en Dagster moderno.** AquÃ­ defines tus *Software-Defined Assets* (SDAs). Cada asset representa una tabla, archivo, u otro objeto de datos persistente. Dagster entiende las dependencias entre assets y puede orquestar su materializaciÃ³n. Se pueden organizar por capa (staging, marts) o dominio.
    * **`ops/`**: (Alternativa/Complemento a Assets) Define *Ops*, que son unidades individuales de cÃ³mputo (funciones Python). Pueden usarse para construir *Jobs*.
    * **`jobs/`**: (Alternativa/Complemento a Assets) Define *Jobs*, que son grafos de *Ops* conectados, representando un pipeline ejecutable.
    * **`resources/`**: Define *Recursos*, que abstraen conexiones a sistemas externos como bases de datos, APIs, etc. Se inyectan en Assets u Ops.
    * **`schedules/` / `sensors/`**: Definen cÃ³mo se disparan tus pipelines (basado en tiempo o eventos).
* **`dbt_project/`**: (Opcional) Un proyecto dbt estÃ¡ndar. Contiene tus modelos SQL (`models/`), tests (`tests/`), macros (`macros/`), etc. Dagster tiene excelente integraciÃ³n para ejecutar `dbt run` o materializar modelos dbt como assets Dagster.
* **`sql/`**: (Alternativa a dbt) Si prefieres no usar dbt, puedes guardar aquÃ­ scripts SQL planos que tus Ops de Python ejecutarÃ¡n.
* **`tests/`**: Pruebas unitarias y de integraciÃ³n para tus Ops, Assets, y Recursos de Dagster usando Pytest.
* **Archivos RaÃ­z:** `pyproject.toml` para dependencias, `README.md`, Dockerfiles opcionales, y archivos de configuraciÃ³n (`.env.example`).

## CÃ³mo Usar esta Plantilla ğŸš€

1.  **Prerrequisitos:**
    * Python (3.8+)
    * Git
    * (Recomendado) Docker y Docker Compose (para ejecutar Dagster UI y dependencias como DBs localmente).
    * (Opcional) Un gestor de paquetes como Poetry o Hatch.
    * (Opcional) dbt CLI si vas a desarrollar modelos dbt localmente (`pip install dbt-core dbt-postgres` o el adaptador que necesites).
2.  **Clonar:** `git clone <URL_DEL_REPO_UNIVERSAL> && cd ejemplos/data-engineering-pipeline`
3.  **Instalar Dependencias Python:**
    * *(Usando Poetry):* `poetry install`
    * *(Usando Hatch):* `hatch env create && hatch shell`
    * *(Usando pip):* `pip install -r requirements.txt` (si existe)
4.  **Variables de Entorno:** Copia `.env.example` a `.env` y configura las variables (credenciales de base de datos, API keys, etc.). Dagster puede cargar variables de `.env`.
5.  **(Opcional - dbt) Configurar Perfil:** Si usas dbt, asegÃºrate de tener un `profiles.yml` configurado (usualmente en `~/.dbt/`) o configura el recurso Dagster para dbt para que lo maneje.
6.  **Iniciar Dagster UI (Dagit) Localmente:** La forma mÃ¡s fÃ¡cil suele ser con Docker Compose si se proporciona un `docker-compose.yml`. Si no, puedes ejecutarlo directamente:
    ```bash
    # AsegÃºrate de estar en el entorno virtual si no usas Hatch/Docker
    # Ejecuta Dagit apuntando al repositorio definido en dagster_project/repository.py
    dagster dev
    ```
    Abre tu navegador en `http://localhost:3000`.
7.  **Explorar y Ejecutar:**
    * Navega por los **Assets** en la UI de Dagit. VerÃ¡s el grafo de dependencias.
    * Selecciona assets y haz clic en **"Materialize"** para ejecutar las Ops o comandos dbt necesarios para crearlos o actualizarlos.
    * Explora los **Jobs** (si estÃ¡n definidos) y ejecÃºtalos.
    * Revisa los **Schedules** y **Sensors**.
    * Inspecciona los **Runs** para ver logs y resultados.
8.  **Ejecutar Pruebas:**
    ```bash
    pytest
    # O usando Poetry/Hatch: poetry run pytest / hatch run test
    ```
    * Si usas dbt, tambiÃ©n puedes ejecutar: `dbt test` (dentro del directorio `dbt_project/` o vÃ­a un Op de Dagster).

## Conceptos Clave (Data Engineering & Dagster) ğŸ’¡

* **ETL vs ELT:** ETL (Extract, Transform, Load) transforma los datos *antes* de cargarlos al destino final (ej: data warehouse). ELT (Extract, Load, Transform) carga los datos crudos o semi-procesados al destino y *luego* los transforma allÃ­ (a menudo usando SQL/dbt). Esta estructura puede soportar ambos.
* **OrquestaciÃ³n:** Herramientas como Dagster gestionan dependencias entre tareas, scheduling, reintentos, alertas y monitorizaciÃ³n, crucial para pipelines complejos.
* **Software-Defined Assets (SDAs):** El enfoque principal de Dagster. Modelas tus pipelines declarando los *activos de datos* que producen (tablas, archivos, reportes, modelos ML) y cÃ³mo se generan. Dagster infiere las dependencias y orquesta su creaciÃ³n/actualizaciÃ³n.
* **Observabilidad y Linaje:** Dagster UI proporciona visibilidad sobre ejecuciones, estado de assets, y linaje de datos (cÃ³mo se generÃ³ un asset).
* **Testing:** Es vital testear la lÃ³gica de transformaciÃ³n (tests unitarios en Ops Python, `dbt test` para modelos SQL) y la calidad de los datos (Great Expectations, Soda Core, dbt tests, Dagster Asset Checks).

## Siguientes Pasos y AdaptaciÃ³n ğŸ‘£

* **Conecta tus Fuentes y Destinos:** Adapta los `resources/` y las Ops/Assets de ingestiÃ³n/carga para tus bases de datos, APIs, o data lakes especÃ­ficos.
* **Desarrolla tus Transformaciones:** Escribe tu lÃ³gica en Python dentro de `ops/` o `assets/`, o crea/modifica modelos en `dbt_project/`.
* **Implementa Chequeos de Calidad:** Usa `dbt test` o integra librerÃ­as como Great Expectations o Soda Core usando Ops de Dagster, o usa los `Asset Checks` nativos de Dagster.
* **Configura Despliegue:** Containeriza tu cÃ³digo de usuario Dagster (`Dockerfile`) y despliÃ©galo en la nube (Kubernetes, ECS, Dagster Cloud) usando CI/CD.
* **Define Schedules/Sensors:** Configura tus pipelines para que se ejecuten automÃ¡ticamente.

## Contribuciones a este Ejemplo âœ¨

Si tienes sugerencias para mejorar *esta plantilla especÃ­fica de data engineering*, por favor sigue la guÃ­a de contribuciÃ³n general (`comunidad/como-contribuir.md`) indicando que tu sugerencia es para `ejemplos/data-engineering-pipeline`.

Â¡Construye pipelines de datos robustos y observables con esta base! ğŸ’§âš™ï¸

```bash

ejemplos/data-engineering-pipeline/
â”œâ”€â”€ .env.example                # Variables de entorno (conexiones DB, API keys)
â”œâ”€â”€ .gitignore                  # Ignorar Python cache, virtualenvs, secrets, logs
â”œâ”€â”€ .dockerignore               # (Opcional) Archivos a ignorar en el build de Docker
â”œâ”€â”€ Dockerfile                  # (Opcional) Para containerizar el cÃ³digo de usuario Dagster
â”œâ”€â”€ docker-compose.yml          # (Opcional) Para correr Dagster UI, daemon, DB, etc. localmente
â”œâ”€â”€ dagster_project/            # Directorio principal del proyecto Dagster (o tu nombre preferido)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ assets/                 # DefiniciÃ³n de Software-Defined Assets (Tablas, Archivos)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ core/               # Ej: Activos de datos 'core'
â”‚   â”‚   â”‚   â””â”€â”€ users.py
â”‚   â”‚   â”œâ”€â”€ marts/              # Ej: Activos de data marts finales
â”‚   â”‚   â”‚   â””â”€â”€ daily_activity.py
â”‚   â”‚   â””â”€â”€ staging/            # Ej: Activos de staging (limpieza inicial)
â”‚   â”‚       â””â”€â”€ stg_orders.py
â”‚   â”œâ”€â”€ jobs/                   # (Opcional) DefiniciÃ³n explÃ­cita de Jobs (grafos de Ops)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ daily_etl_job.py
â”‚   â”œâ”€â”€ ops/                    # (Opcional) DefiniciÃ³n explÃ­cita de Ops (unidades de cÃ³mputo)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”‚   â””â”€â”€ fetch_api_data.py
â”‚   â”‚   â””â”€â”€ transformation/
â”‚   â”‚       â””â”€â”€ clean_data.py
â”‚   â”œâ”€â”€ resources/              # DefiniciÃ³n de Recursos (conexiones DB, clientes API)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ db_resource.py
â”‚   â”œâ”€â”€ schedules/              # DefiniciÃ³n de Schedules para ejecutar Jobs/Assets
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ daily_schedule.py
â”‚   â”œâ”€â”€ sensors/                # (Opcional) DefiniciÃ³n de Sensors (ej: reaccionar a archivos S3)
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ repository.py           # Punto de entrada: Define el repositorio de cÃ³digo Dagster
â”œâ”€â”€ dbt_project/                # (Opcional) Proyecto dbt estÃ¡ndar para transformaciones SQL
â”‚   â”œâ”€â”€ dbt_project.yml         # ConfiguraciÃ³n del proyecto dbt
â”‚   â”œâ”€â”€ models/                 # Modelos dbt (archivos .sql)
â”‚   â”‚   â”œâ”€â”€ marts/
â”‚   â”‚   â”‚   â””â”€â”€ core/
â”‚   â”‚   â”‚       â””â”€â”€ dim_users.sql
â”‚   â”‚   â””â”€â”€ staging/
â”‚   â”‚       â”œâ”€â”€ schema.yml      # DocumentaciÃ³n y tests de schema dbt
â”‚   â”‚       â””â”€â”€ base/
â”‚   â”‚           â””â”€â”€ stg_orders.sql
â”‚   â”œâ”€â”€ profiles.yml            # (Usualmente en ~/.dbt/ o gestionado por Dagster resource)
â”‚   â”œâ”€â”€ snapshots/              # (Opcional) Snapshots dbt
â”‚   â””â”€â”€ tests/                  # (Opcional) Tests singulares dbt
â”œâ”€â”€ notebooks/                  # (Opcional) ExploraciÃ³n y anÃ¡lisis ad-hoc
â”‚   â””â”€â”€ data_profiling.ipynb
â”œâ”€â”€ pyproject.toml              # Dependencias Python (dagster, pandas, dbt-core, etc.), build, tools
â”œâ”€â”€ README.md                   # Este archivo: explicaciÃ³n del ejemplo
â”œâ”€â”€ scripts/                    # (Opcional) Scripts auxiliares (setup, limpieza)
â”‚   â””â”€â”€ setup_db.sh
â”œâ”€â”€ sql/                        # (Alternativa a dbt) Scripts SQL planos usados por Ops Python
â”‚   â””â”€â”€ transformations/
â”‚       â””â”€â”€ aggregate_orders.sql
â””â”€â”€ tests/                      # Pruebas Python para Ops, Assets, Recursos
    â”œâ”€â”€ conftest.py
    â”œâ”€â”€ test_assets.py
    â””â”€â”€ test_ops.py

```